---
title: "Intro to Machine Learning (Assignment 4)"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_document
---
```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(readr)
library(caret)
library(stats)
library(modelr)


set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

<hr>
## PART 1: Implementing a Simple Prediction Pipeline 

Using the dataset class4_p1.csv, we will fit and evaluate two prediction models 
using linear regression. The aim of the models are to predict the number of days 
in a month an individual reported having good physical health (feature name: healthydays).

Let's first read in our dataset and perform some data cleaning and manipulations.

```{r}
df_part1 <-
  read_csv("data/class4_p1.csv", col_names = TRUE) %>% 
    janitor::clean_names() %>% 
# Let's check to see summary statistics and quick glimpse of our data.
   glimpse()
```
From the output above, we notice that all our features are stored as 'dbl',
continuous variables. Therefore, we will use the output above in addition to the 
codebook to store our features in the intended format. In addition we will also
remove all missing values. We remove the variable "x1" that has the count of the 
number of observations in the dataset.

```{r}
df_part1 <-
  df_part1 %>% 
    # Delete 'x1' that shows number of observations in dataset (serves no purpose for us)
  select(-x1) %>% 
  ## Converting categorical variables to factor variables
  mutate(
     chronic1 = as.factor(chronic1), 
     chronic3 = as.factor(chronic3),
     chronic4 = as.factor(chronic4),
     tobacco1 = as.factor(tobacco1), 
     alcohol1 = as.factor(alcohol1),
     habits5 = as.factor(habits5),
     habits7 = as.factor(habits7),
     agegroup = as.factor(agegroup),
     dem3 = as.factor(dem3),
     dem4 = as.factor(dem4),
     dem8 = as.factor(dem8),
     povertygroup = as.factor(povertygroup)
  ) %>% 
  # Remove all missing values
  na.omit()


  # Let's quickly look at our dataset and the new feature types 
  summary(df_part1)
```
Our categorical (factor) variables are: <br> 
chronic1, chronic3, chronic4, tobacco1, alcohol1, habits5, habits7,
agegroup, dem3, dem4, dem8 and povertygroup.

Our continuous variables are: <br>
bmi, gpaq8totmin, gpaq11days and healthydays. 


### Next, let's partition our dataset into training and testing using the caret package.

```{r}
  #Create balanced partitons and ensure balance of outcome (healthydays
partition_df <-createDataPartition(df_part1$healthydays, p=0.7, list=FALSE)
  
  #Let's create our training set from this partition
df_train <- df_part1[partition_df,]

  #Let's create our testing set from this partition
df_test <- df_part1[-partition_df,]

  #Let's view number of rows in each set 
NROW(df_train) 
NROW(df_test)

```

The results above show that our training dataset contains 1,537 rows, which accounts for
roughly 70% of the original dataset. Also, we can see that our testing dataset contains 
658 rows, which roughly account for 30% of the original dataset. 
We can be confident that we correctly partitioned our original dataset in the manner
we would like, a 70/30 split.


1. Now let's fit two prediction models using different subsets of the features in the 
training data.The aim of the models are to predict the number of days in a month an 
individual reported having good physical health (feature name: healthydays) <br>
## Do I randomly select features or look at correlation matrix ?!

In our first model, we will select the following features to predict 'healthydays' :
dem3 + dem8 + agegroup + gpaq8totmin + habits5 + habits7

```{r}
#Model 1
lin_model_fit1 <-
  df_train %>% 
     lm(healthydays ~ dem3 + dem4 + dem8 + agegroup + gpaq8totmin + habits5 + habits7 + 
          tobacco1 + alcohol1, data = .) 

```


In our second model we will select the following features to predict 'healthydays':
dem3 + agegroup + bmi + chronic1 + chronic3 + chronic4 + povertygroup

```{r}
#Model 2
lin_model_fit2 <-
  df_train %>% 
     lm(healthydays ~ dem3 + agegroup + bmi + chronic1 +  gpaq11days +
          chronic3 + chronic4 + povertygroup, data = .) 

```

2. Now let's apply both linear models fitted above to our test data. We will also 
determine the preferred model to use for predicting 'healthdays' using the Mean 
Square Error. We will present the MSE visually. 

```{r}
# Utilize root mean square error here to evaluate our model
rmse(lin_model_fit2, df_test)
rmse(lin_model_fit1, df_test)

rsquare(lin_model_fit2, df_test)
rsquare(lin_model_fit1, df_test)

summary(lin_model_fit1)$adj.r.squared
summary(lin_model_fit2)$adj.r.squared
```
### Do the step above in 1 step and make graph as well 

From the application of the root mean square error in our regression models, 
we can see that model 1 has the lowest mean square error indicating that this 
model is better compared to model2 at predicting 'healthydays'.  

3. One setting where the implementation of the model selected here would be useful
is in the clinical setting at primary care visits for adults. This model could help 
doctors make recommendations about how their patients improve or have helathier days 
in a month based on the features in the model.

<hr>
## PART 2: Conducting an Unsupervised Analysis


Using the built-in R dataset USArrests we identified clusters using hierarchical analysis. 
In addtiion,, we Used an agglomerative algorithm for hierarchical clustering and a 
Euclidian distance measure to construct our dissimilarity matrix.

```{r}
## Read in USArrests dataset and save as df_part2
  data("USArrests") 

  df_part2 <-
    USArrests %>% 
    as_tibble() %>% 
    janitor::clean_names()
  
## Let's see roughly what this dataset looks like and check to see if any missing values exist 
  glimpse(df_part2)
```
From the 'glimpse' function, we see that all our features are of a continuous data type.

```{r}
## Let's check to see if there are any missing values for our features 
which(is.na(df_part2))
```
It appears that there aren't any missing values in our dataset. For this problem, 
we would like to eliminate missing values.
 
```{r}
## Let's check the means and standard deviations of our features 
colMeans(df_part2)
apply(df_part2, 2, sd, na.rm=TRUE)
```
By checking the means and standard deviations of our features, we notice that their
values are not similar. Therefore, we need to center and scale our features before 
performing clustering so that we have a standard deviation of 1 and a mean of zero.
 
```{r}
## Let's perform scaling and centering here 
df_part2 <-
  df_part2 %>% 
    scale(center=TRUE, scale = TRUE)

## Let's check the new mean and std after the scaling process
colMeans(df_part2)
apply(df_part2, 2, sd, na.rm=TRUE)
```
4. We now see that our means are similar in addition to our standard deviation. We 
will now proceed to perform our hierarchical clustering. <br>
First, we had to decided cluster dissimilarity. 
We used a "euclidean" method to calculate the distance and performed 
hierarchical clustering using "complete linkage".

```{r}
## Let's create a dissimilarity matrix 
part2_diss_matrix <- 
  dist(df_part2, method = "euclidean")

# Let's perform agglomerative Hierarchical clustering using Complete Linkage
clusters_fit <- 
  hclust(part2_diss_matrix, method = "complete" )

# Let's plot the obtained dendrogram
plot(clusters_fit, cex = 0.6, hang = -1)
```


4. 

